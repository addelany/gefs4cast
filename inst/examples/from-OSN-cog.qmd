---
output: html
---


```{r}
library(arrow)
library(stars)
library(glue)
library(tidyverse)
library(bench)
```


Ideally we would get all necessary metadata (urls, bands, and datetimes) from STAC, but a poor man can parse the file names instead...

```{r}
get_catalog <- function(date = Sys.Date(),
                      bucket =  "bio230014-bucket01", 
                      endpoint = "sdsc.osn.xsede.org", # note the SDSC region
                      path = "neon4cast-drivers/noaa/gefs-v12/cogs") {
  
  date <- as.Date(date)
  day <- stringr::str_remove_all(date, "-")
  fspath <- glue::glue(path, "gefs.{day}", .sep="/")
  
  s3 <- arrow::s3_bucket(bucket, endpoint_override = endpoint, anonymous=TRUE)
  files <- s3$path(fspath)$ls(recursive=TRUE)

  meta <- tibble::tibble(files) |>
    tidyr::separate("files",
                    into=c("prefix", "cycle", "type", "res", "horizon", "ext")) |>
    dplyr::mutate(horizon = stringr::str_extract(horizon, "\\d{3}"),
           datetime = date + lubridate::hours(horizon),
           url = paste("/vsicurl/https:/", endpoint, bucket, fspath, files, sep="/"))
  
  meta
}
```



```{r}
date <- as.Date("2022-01-01")
meta <- get_catalog(date)
m <- meta |> dplyr::filter(prefix=="geavg", horizon != "000")  # note 000 has different bandnames
```



# stars

`stars` is a spatio-temporal asset

```{r}

bench_time({
  x <- stars::read_stars(m$url, along= list(time = m$datetime))
})

sites <- read_csv(paste0("https://github.com/eco4cast/neon4cast-noaa-download/",
                         "raw/master/noaa_download_site_list.csv"))
sf_sites <- sf::st_as_sf(sites,coords=c("longitude", "latitude"), crs = 4326) |>
  tibble::rowid_to_column("FID") |>
  st_transform(st_crs(x))


bench_time({
  df2 <- stars::st_extract(x, sf_sites)
})

```


# gdalcubes



gdalcubes offers greater flexibility and performance.  First we must aggregate images into an image collection with appropriate metadata about bands and times.

```{r}
library(gdalcubes)


get_bandnames <- function(meta) {
 stars::read_stars(meta$url[1]) |>
 stars::st_get_dimension_values(3)
}

bandnames <- get_bandnames(m)

gefs_cube <- create_image_collection(m$url,
                                     date_time = m$datetime,
                                     band_names = get_bandnames(m))


```


```{r}
# USA bbox
box <- c(-125, 25, -66, 49)
iso <-"%Y-%m-%dT%H:%M:%S"
ext <- list(t0 = as.character(min(m$datetime),iso), 
            t1 = as.character(max(m$datetime),iso),
            left = box[1], right = box[3],
            top = box[4], bottom = box[2])

v <- cube_view(srs = "EPSG:4326", 
               extent = ext,
               dx = 0.5, dy = 0.5, # original resolution -- half-degree
               dt = "PT3H",   # original resolution
               aggregation = "mean", resampling = "cubicspline"
)

```


## Extract timeseries at sites

```{r}
sites <- readr::read_csv(paste0("https://github.com/eco4cast/neon4cast-noaa-download/",
                "raw/master/noaa_download_site_list.csv"))
sf_sites <- sf::st_as_sf(sites,coords=c("longitude", "latitude"), crs = 4326) |>
  tibble::rowid_to_column("FID")



## gdalcubes extraction
bench::bench_time({
df <- raster_cube(gefs_cube, v) |>
  extract_geom(sf_sites) |>
  as_tibble() |>
  inner_join(sf_sites)
})


## tada
df |> filter(site_id == "ABBY") |> 
  mutate(time = lubridate::as_datetime(time)) |>
  ggplot(aes(time,TMP)) + geom_point()
```


on-the-fly calculation for animation:

```{r}
## Lets have some fun
raster_cube(gefs_cube, v) |>
  select_bands("TMP") |>
  gdalcubes::fill_time(method="linear") |> 
  animate( col = viridisLite::viridis, nbreaks=100,
           fps=10, save_as = "temp.gif")
```
